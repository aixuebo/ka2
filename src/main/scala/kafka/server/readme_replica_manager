ReplicaManager 管理该节点上partition的类
作用
1.一个partition对应一个partition对象---每一个partition对象包含了他所有的备份节点对象
2.接受leaderAndISRRequest请求,通知该节点上存在哪些partition,以及这些partition的leader和备份节点集合详细信息
3.接受StopReplicaRequest请求,对一组topic-partition要在该节点上删除掉,即该节点不再处理这些partition信息了
4.接受UpdateMetadataRequest请求,处理更新元数据请求
5.接受FetchRequest抓去请求,follower节点来本地节点获取数据

一、构造函数需要的主要对象
1.LogManager 用于在本节点上对一个partiton创建一个日志文件,存储该partition的内容,一个partition理论上在一个节点上只是会出现一个,不然log对象会冲突的。
 该partition的日志可以是leader的日志文件,也可以使follow的文件,从leader节点同步过来的数据存储在本地。
2.ZkClient 连接zookeeper

二、属性
1.private val allPartitions = new Pool[(String, Int), Partition]//就是一个Map,通过(String, Int)参数可以返回一个Partition对象
  key是topic-partition组成,value是组成的Partition对象,该对象管理这副本备份信息
2.@volatile var controllerEpoch: Int = KafkaController.InitialControllerEpoch - 1
  表示controller的最新的选举次数,默认是很小的值,当每次请求过来的时候,都会与该值进行校验,然后更新该值,因此当第一个请求过来的时候,该值就会被重新赋值,就是最新的controller选举次数
3.private val localBrokerId = config.brokerId//本地属于哪个节点ID
4.private var hwThreadInitialized = false //true表示已经打开了一个线程,定期向replication-offset-checkpoint文件写入数据
  只有第一次收到leaderAndISRRequest请求后,才会开启线程,防止没有partition的数据,空向线程写入数据




三、创建partition和Parttion对象的映射关系
使该节点上知道有一个partition在该节点上存在了
1.getOrCreatePartition(topic: String, partitionId: Int): Partition 创建或者get一个partition对象
 new Partition(topic, partitionId, time, this) 创建的时候,partition会持有该ReplicaManager对象引用
2.getPartition(topic: String, partitionId: Int): Option[Partition] 只是get,不会创建
3.def getReplica(topic: String, partitionId: Int, replicaId: Int = config.brokerId): Option[Replica] 获取该partition的一个备份信息
 通过topic和partitionId,获取Partition对象---通过replicaId获取在replicaId节点上的备份对象信息,默认获取本节点的备份对象
4.def getReplicaOrException(topic: String, partition: Int): Replica
  获取本节点上的备份对象,如果不存在则抛异常
5.def getLeaderReplicaIfLocal(topic: String, partitionId: Int): Replica
   获取本节点上的备份对象,如果不存在则抛异常,如果存在,但不是leader节点,也抛异常

四、becomeLeaderOrFollower(leaderAndISRRequest: LeaderAndIsrRequest,offsetManager: OffsetManager): (collection.Map[(String, Int), Short], Short)
接受leaderAndISRRequest请求,通知该节点上存在哪些partition,以及这些partition的leader和备份节点集合详细信息
1.先打印日志,说明此时该partition的详细信息
2.校验该controller的请求是否合法
3.更新controller的最新的选举次数
 controllerEpoch = leaderAndISRRequest.controllerEpoch //当前controller枚举次数
4.val partitionState = new HashMap[Partition, PartitionStateInfo]() 更新每一个partition对应的leader的详细信息
循环所有的partition请求,为每一个partition(创建/get)一个Partition对象,即有则获取,无则创建,并且存储到partitionState中,这样每一个partition就有了请求的partition详细信息对应关系了
注意:这一步骤不是把所有的请求partition都加入映射,要过滤掉partition的leader枚举次数非法的,如果partition的备份节点并没有该节点,则也会被过滤掉
5.从4中得到的partition,按照leader节点是不是本节点进行分组,就会产生两个分组.partitionsTobeLeader本节点是partition的leader节点、partitionsToBeFollower本节点是partition的follow节点
6.然后分别处理这两个分组------参见五和六
makeLeaders(controllerId, controllerEpoch, partitionsTobeLeader, leaderAndISRRequest.correlationId, responseMap, offsetManager)
makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, leaderAndISRRequest.leaders, leaderAndISRRequest.correlationId, responseMap, offsetManager)
7.只有第一次收到leaderAndISRRequest请求后,才会开启线程,防止没有partition的数据,空向线程写入数据
  开启线程,定期向replication-offset-checkpoint文件写入信息
8.replicaFetcherManager.shutdownIdleFetcherThreads()
  将闲置的线程进行shutdown

五、makeLeaders(controllerId: Int,//controller节点ID,即哪个节点发过来的请求
                            epoch: Int,//controller的枚举次数
                            partitionState: Map[Partition, PartitionStateInfo],//当前要在该节点做leader的partition和leader的详细信息映射
                            correlationId: Int,//客户端发送过来的请求的关联ID
                            responseMap: mutable.Map[(String, Int), Short],//为每一个有异常的topic-partition提供异常状态码
                            offsetManager: OffsetManager)
partitionState集合内所有topic-partitio的leader节点都是本节点
1.打印每一个partition日志,说明该partition在该节点已经是leader了,开始处理逻辑
2. replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(new TopicAndPartition(_)))
  停止抓取这些partition,因为已经是leader了,不需要抓数据了
3.partitionState.foreach{ case (partition, partitionStateInfo) => partition.makeLeader(controllerId, partitionStateInfo, correlationId, offsetManager)}
 通知每一个partition对象,该partition在该节点已经是leader了
 立刻看partition类的makeLeader方法
4.打印日志每一个partition已经完成了处理

六、def makeFollowers(controllerId: Int,//controller的节点ID
                              epoch: Int, //controller的枚举次数
                              partitionState: Map[Partition, PartitionStateInfo],//follow节点的集合
                              leaders: Set[Broker], //leader节点集合
                              correlationId: Int, //request请求的关联ID
                              responseMap: mutable.Map[(String, Int), Short],//返回给controller的每一个topic-partition的状态码
                              offsetManager: OffsetManager) {
该partition在该节点是follow节点,说明在该节点上一定要有日志存储信息
1.打印日志,这些partition在本节点上要开始做follower了
2. var partitionsToMakeFollower: Set[Partition] = Set()存储已经变更了leader节点的partition
循环每一个partition---如果该partition的leader节点是在leader节点集合中,并且leader节点有变化了,则添加到集合中
3.停止2中所有的partition的抓去工作,因为leader已经更换了,因此以前抓去的地方已经错误了,因此要先停止抓去
4.将topic-partition在本地的备份log进行截断,截断到该备份对象里面存储的该备份同步leader到哪个位置了
 其实不是特别理解为什么要截断,如果以前是leader,现在变成follow了,或者以前向leader1进行同步,现在向leader2进行同步,但是为什么要截断呢
 其实我目前来看,最重要的就是更新日志的位置,下一次抓去的时候从该位置开始抓去,但是不截断，也是可以从该位置开始更新的啊。为什么要截断操作呢
5.对这些topic-partition重新向新的leader进行抓去数据
 如果服务器已经shutdown了,则不需要再进行更新leader数据了,否则要重新连接新的leader,进行数据抓取
6.打印日志每一个partition已经完成了处理

七、def stopReplica(topic: String, partitionId: Int, deletePartition: Boolean): Short
该节点不再存储该topic-partition备份信息
1.删除partition和partition对象的映射
2.删除本地关于该partition的日志文件
八、def stopReplicas(stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicAndPartition, Short], Short)
对一组topic-partition要在该节点上删除掉,即该节点不再处理这些partition信息了
1.首先停止抓去这些partition,因为都要删除了,没有必要在去抓去这些了
2.循环每一个partition调用七方法进行删除

九、def maybeUpdateMetadataCache(updateMetadataRequest: UpdateMetadataRequest, metadataCache: MetadataCache)
接受UpdateMetadataRequest请求,处理更新元数据请求,让每一个节点知道controller节点上缓存一下此时的topic-partition的详细信息
1.更新controoler的枚举次数
2.更新缓存信息metadataCache.updateCache(updateMetadataRequest, localBrokerId, stateChangeLogger)
a.topic-partition-PartitionStateInfo映射关系
b.缓存活着的broker节点信息

十、def readMessageSet(topic: String,//去读取该topic-partition数据
                               partition: Int,
                               offset: Long,//从offset位置开始读取
                               maxSize: Int,//最多读取多少个字节
                               fromReplicaId: Int) //请求来源于哪个follow节点
                              : (FetchDataInfo, Long) 从本地读取该topic-partition的数据,从offset位置开始读取,最多读取maxSize个字节
1.因为该方法是follower节点向leader节点调用的,因此该方法一定是在leader的节点上执行的,因此可以获取到leader节点对应的备份对象,即一定是存在本地log文件
val localReplica = getLeaderReplicaIfLocal(topic, partition) Replica获取该topic-partition对应的本地文件
2.读取本地文件log.read(offset, maxSize, maxOffsetOpt)  返回值FetchDataInfo
3.最终返回读取的信息数据给follow,(fetchInfo, localReplica.highWatermark.messageOffset)
返回值localReplica.highWatermark.messageOffset的含义
要告诉每一个客户端follower节点,当前leader已经同步到哪些位置了,即所有的同步集合都至少也拿到了该位置的数据了。该位置之前的数据都已经全部同步完成

十一、def readMessageSets(fetchRequest: FetchRequest)
接受FetchRequest抓去请求,follower节点来本地节点获取数据
1.依次调用十方法,抓去每一个topic-partition的数据
2.将抓去后的结果返回给follow或者客户端

十二、def getLeaderPartitions() : List[Partition]
在本节点的所有topic-partition中获取所有是leader的partition集合

十三、def updateReplicaLEOAndPartitionHW(topic: String, partitionId: Int, replicaId: Int, offset: LogOffsetMetadata)
FetchRequest发送请求,读取了信息后,会调用该方法,更新leader所在节点,让leader知道他的follow节点都已经同步到什么程度了
参数说明topic-partition的在replicaId节点上的follow信息已经更新到LogOffsetMetadata位置了,LogOffsetMetadata位置表示此时follow请求的开始位置,即说明该位置follow节点已经存在了
1.找到该follow节点对应的Replica对象,更新他的logEndOffset信息 replica.logEndOffset = offset
2.partition.updateLeaderHWAndMaybeExpandIsr(replicaId)
更新leader的信息以及可能扩展该备份节点到同步节点集合中,如果该follow节点进度追赶的达到一定指标了,则将其加入到同步集合中

十四、def startup()
核心方法,启动入口函数
该方法用线程定期执行以下代码,定期收缩每一个partition的同步节点集合,让进度慢的或者死掉的follow节点从同步节点集合中移除
allPartitions.values.foreach(partition => partition.maybeShrinkIsr(config.replicaLagTimeMaxMs, config.replicaLagMaxMessages))
1.循环每一个partition,执行每一个partition的maybeShrinkIsr方法
2.该方法会将长时间不能请求leader,来抓去信息的partition从同步集合中移除
  该方法会将与leader的数据差异很大,即follower节点更新很慢的,从同步集合中移除
十五、def startHighWaterMarksCheckPointThread()
在第一次接收到leaderAndISRRequestde请求后,会产生一个线程,定期向replication-offset-checkpoint文件写入每一个topic-paritition对应的处理过的文件偏移量
1.val replicas = allPartitions.values.map(_.getReplica(config.brokerId)).collect{case Some(replica) => replica}
获取本地有log记录的备份数据对象集合
2.val replicasByDir = replicas.filter(_.log.isDefined).groupBy(_.log.get.dir.getParentFile.getAbsolutePath)
Map[String, Iterable[Replica]] 按照log的根目录分组,key是log根目录,value是该目录下的所有Replica对象
按照log所在目录进行分组,每一个目录下有哪些备份集合
3.循环每一个目录下的备份集合
 将每一个partition本地的备份Replica对象的highWatermark.messageOffset记录到日志中,即本地节点已经同步到哪个位置了记录到日志中

十六、underReplicatedPartitionCount(): Int
返回在本节点是leader的partition集合中,没有同步完成的partition数量,如果备份集合数量>同步集合数量,则认为没有同步完成

  /**
   * Unblock some delayed produce requests with the request key
   */
  def unblockDelayedProduceRequests(key: TopicAndPartition) {
    val satisfied = producerRequestPurgatory.update(key)
    debug("Request key %s unblocked %d producer requests."
      .format(key, satisfied.size))

    // send any newly unblocked responses
    satisfied.foreach(producerRequestPurgatory.respond(_))
  }

  /**
   * Unblock some delayed fetch requests with the request key
   */
  def unblockDelayedFetchRequests(key: TopicAndPartition) {
    val satisfied = fetchRequestPurgatory.update(key)
    debug("Request key %s unblocked %d fetch requests.".format(key, satisfied.size))

    // send any newly unblocked responses
    satisfied.foreach(fetchRequestPurgatory.respond(_))
  }


--------------------------------------------------
Partition
kafka还可以配置partitions需要备份的个数(replicas),每个partition将会被备份到多台机器上,以提高可用性.
每一个Partition是由topic决定的,即一个topic对应多个Partition,分别编号为partitionId
一、属性
1.private val localBrokerId = replicaManager.config.brokerId//本地节点ID
2.private val logManager = replicaManager.logManager//本地的LogManager,用于将partition的一个备份写到本地节点上,这个备份可能是leader,也可能是follow
3.private var controllerEpoch: Int = KafkaController.InitialControllerEpoch - 1
  controller的枚举次数,当请求过来的时候,设置的该值,每次partition变成leader或者follow的时候都会更新该值。
  @volatile private var leaderEpoch: Int = LeaderAndIsr.initialLeaderEpoch - 1 //leader选举的次数
  private var zkVersion: Int = LeaderAndIsr.initialZKVersion
4.partition的三大属性
   @volatile var leaderReplicaIdOpt: Option[Int] = None//该partition对应的leader节点
   @volatile var inSyncReplicas: Set[Replica] = Set.empty[Replica]//已经同步的备份集合,只有partition的leader节点需要知道该属性
  private val assignedReplicaMap = new Pool[Int, Replica] 保存该partition的每一个备份在哪个节点上,以及该备份对象Replica的映射关系


二、创建该partition与备份的映射关系
1.def getOrCreateReplica(replicaId: Int = localBrokerId): Replica
  有则获取,无则创建一个在replicaId节点上的备份对象
a.如果备份节点不是本节点,则val remoteReplica = new Replica(replicaId, this, time)即可,添加映射
b.如果备份节点就是本地节点,则创建或者获取对应的log文件
c.读取replication-offset-checkpoint文件对象OffsetCheckpoint,读取该topic-partition存储的值
  将该值与log.logEndOffset的进行比较,获取较小的值,创建备份对象
  val localReplica = new Replica(replicaId, this, time, 最小值, Some(log))
d.添加映射
2.def getReplica(replicaId: Int = localBrokerId): Option[Replica] 仅仅是get获取,不会创建
3.def leaderReplicaIfLocal(): Option[Replica]
 获取leader节点的备份对象,并且该备份节点还是得本地节点,否则返回null
 即只有本地是leader的时候,才会返回该备份对象
4.对备份节点ID与备份对象的映射关系方法
 def addReplicaIfNotExists(replica: Replica) 添加
 def assignedReplicas(): Set[Replica]  获取全部的备份
 def removeReplica(replicaId: Int) 删除一个映射
三、def delete() 说明该partition在该本地节点不再存在了,则
1.assignedReplicaMap.clear()清空各种映射
2.删除本地的log文件 logManager.deleteLog(TopicAndPartition(topic, partitionId))
四、def getLeaderEpoch(): Int
返回leader选举的次数
五、 makeLeader(controllerId: Int,//controller节点
                   partitionStateInfo: PartitionStateInfo,//此时该partition的leader详细信息
                   correlationId: Int,//请求关联的ID
                   offsetManager: OffsetManager): Boolean = 说明该partition在本节点是leader存在
说明该partition在该节点是leader了
因为请求过来的是最新的该partition分配在哪些节点上备份,leader在哪个节点上,同步节点集合在哪些节点上,所以都要进行更新的
1.删除全部的内存映射
2.将该partition的所有备份集合分别创建备份对象Replica
3.将该partition的所有同步集合分别创建备份对象Replica
4.设置属性 最新的同步集合、leader节点(本地节点)、leader的选举次数
5.newLeaderReplica.convertHWToLocalOffsetMetadata() 如果本地备份对象是leader的时候,调用备份Replica对象的convertHWToLocalOffsetMetadata方法
6.因为本节点已经是该partiiton的leader节点了,因此本节点该partition的所有的follow都要重置偏移量,即先把所有的follow节点上报的同步到什么位置了设置为空
  assignedReplicas.foreach(r => if (r.brokerId != localBrokerId) r.logEndOffset = LogOffsetMetadata.UnknownOffsetMetadata)
7.maybeIncrementLeaderHW(newLeaderReplica)
 因为没有同步节点数据,因此暂时说明同步节点大家都没有开始同步
 当下次follow节点以此来的时候,就知道谁是最小的了,就可以设置该值了
8.if (topic == OffsetManager.OffsetsTopicName) //kafka内部的topic,单独加载
          offsetManager.loadOffsetsFromLog(partitionId)

六、def makeFollower(controllerId: Int,//controller节点ID
                     partitionStateInfo: PartitionStateInfo,//此时poartition的leader详细信息
                     correlationId: Int,//请求关联的ID
                     offsetManager: OffsetManager): Boolean
说明该partition在该节点是follower了,如果leader节点没有被更改,则返回false给replica manager
1.更新controller的选举次数
2.更新partition的leader枚举次数
3.更新partition的同步节点集合是空集合,因为follow节点是不需要知道所有的同步节点有哪些的
4.为该partition所有的备份节点创建一个备份对象集合,清空以前的备份集合映射,以最新的为准
5.判断该parititon最新的leader节点是否与以前存储的相同,如果相同,说明leader没有变化,则返回false,leader有变化则返回true,并且更新leader

七、def isUnderReplicated(): Boolean
 返回 inSyncReplicas.size < assignedReplicas.size
 返回在partition是否同步完成,如果备份集合数量>同步集合数量,则认为没有同步完成

八、def updateLeaderHWAndMaybeExpandIsr(replicaId: Int)
对leader节点进行更新,并且扩展同步节点集合
1.获取本地partition的备份,并且本地必须是leader
2.获取参数备份节点对应的备份对象
val replica = getReplica(replicaId).get //返回一个备份节点对应的Replica对象
3.如果当前备份参数备份节点不再同步节点集合 && 参数备份节点ID是该partition在分配的备份节点ID集合中 && 参数备份节点在leader上收到的抓去请求开始位置已经是leader的最后位置了
说明已经是同步完成了
因此更新同步集合,并且同步到zookeeper上
4.maybeIncrementLeaderHW(leaderReplica) 方法调用


九、def updateIsr(newIsr: Set[Replica])
对leader节点更新对应的同步节点集合,仅仅改变该partition的同步节点集合
将同步信息更新到zookeeper上

十、maybeIncrementLeaderHW(leaderReplica)
只有leader节点才能调用该方法
可能更改leader节点所在备份对象的highWatermark值
1.对所有同步节点集合获取已经收到同步到什么程度了,并且获取最小的同步节点同步到哪个位置了
val allLogEndOffsets = inSyncReplicas.map(_.logEndOffset) //获取每一个follow对象的位置
val newHighWatermark = allLogEndOffsets.min(new LogOffsetMetadata.OffsetOrdering) //获取最小值
2.获取leader的highWatermark位置值,与最新的同步最小值做比较,如果该值<highWatermark,则说明leader已经又同步了一些位置了
因此重新设置leaderReplica.highWatermark = newHighWatermark
然后调用(这块不是很理解,这个操作的意义是什么)
      val requestKey = new TopicAndPartition(this.topic, this.partitionId)
      replicaManager.unblockDelayedFetchRequests(requestKey)
      replicaManager.unblockDelayedProduceRequests(requestKey)

十一、def getOutOfSyncReplicas(leaderReplica: Replica, keepInSyncTimeMs: Long, keepInSyncMessages: Long): Set[Replica]
 返回卡住的同步对象集合
 所谓卡住的原因是:1.长时间没有从leader收到同步信息 2.收到的leader的同步信息数据较少
1.val leaderLogEndOffset = leaderReplica.logEndOffset 获取leader的log本地最后一个位置
2.val candidateReplicas = inSyncReplicas - leaderReplica //刨除leader的同步集合 Set[Replica]
3.查找不卡住的备份对象,即长时间没有同步信息
val stuckReplicas = candidateReplicas.filter(r => (time.milliseconds - r.logEndOffsetUpdateTimeMs) > keepInSyncTimeMs)
4.已经落后leader的数据已经很久了
 val slowReplicas = candidateReplicas.filter(r =>
      r.logEndOffset.messageOffset >= 0 &&
      leaderLogEndOffset.messageOffset - r.logEndOffset.messageOffset > keepInSyncMessages)

十二、def maybeShrinkIsr(replicaMaxLagTimeMs: Long,  replicaMaxLagMessages: Long)
1.调用十一,找到已经过期的备份集合
2.val newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas //刨除有问题的备份集合,剩余可用的集合
3.updateIsr(newInSyncReplicas) 更新同步集合的zookeeper信息
4.maybeIncrementLeaderHW(leaderReplica) 更新leader同步的最后一个位置

十三、def appendMessagesToLeader(messages: ByteBufferMessageSet, requiredAcks: Int=0)
向本地的partition的leader中追加一条ByteBufferMessageSet信息
1.在该节点上找到leader的备份对象,一定会有log对象存在
2.校验同步集合的数量是否满足,不满足则抛异常
3.val info = log.append(messages, assignOffsets = true) 追加信息到leader所在的日志文件中
4.replicaManager.unblockDelayedFetchRequests(new TopicAndPartition(this.topic, this.partitionId)) 暂时不太理解
5.maybeIncrementLeaderHW(leaderReplica) 增加该leader的同步位置

十四、def checkEnoughReplicasReachOffset(requiredOffset: Long, requiredAcks: Int): (Boolean, Short)
必须在leader节点进行处理,校验该leader节点的备份数量是否达到标准
1.获取leader对应的备份对象
2.校验备份数量是否达到标准
